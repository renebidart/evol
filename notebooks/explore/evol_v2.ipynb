{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rbbidart/learn-lr/src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.pyplot import imshow\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.metrics import accuracy_score\n",
    "% matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.sampler as sampler\n",
    "\n",
    "# Import modules every time \"you run code imported using %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# Add the src directory for functions\n",
    "src_dir = os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), 'src')\n",
    "print(src_dir)\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "# import my functions:\n",
    "from models import*\n",
    "from genetic import*\n",
    "from scheduler import*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check what is going on with the learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.current_device():  0\n",
      "torch.cuda.get_device_name(0):  Tesla P100-PCIE-12GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rbbidart/learn-lr/src/models.py:112: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Epoch:  1\n",
      "Learning Rate:  [1e-05]\n",
      "train loss:  2.32293\n",
      "valid_loss:  2.3075744234720865 valid_acc:  9.825\n",
      "-------Epoch:  2\n",
      "Learning Rate:  [1e-05]\n",
      "train loss:  2.31692\n",
      "valid_loss:  2.30682786432902 valid_acc:  9.841666666666667\n",
      "-------Epoch:  3\n",
      "Learning Rate:  [1e-05]\n",
      "train loss:  2.31606\n",
      "valid_loss:  2.306082009633382 valid_acc:  9.858333333333333\n",
      "-------Epoch:  4\n",
      "Learning Rate:  [0.01]\n",
      "train loss:  0.812564\n",
      "valid_loss:  0.4022487718264262 valid_acc:  89.53333333333333\n",
      "-------Epoch:  5\n",
      "Learning Rate:  [0.01]\n",
      "train loss:  0.56451\n",
      "valid_loss:  0.27721950531005857 valid_acc:  92.66666666666667\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "data_loc = '/home/rbbidart/project/rbbidart/learn-lr/data'\n",
    "batch_size =128\n",
    "momentum =.5\n",
    "epochs = 5\n",
    "schedule = [.00001, .00001, .00001, .01, .01]\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "print('torch.cuda.current_device(): ', torch.cuda.current_device())\n",
    "print('torch.cuda.get_device_name(0): ', torch.cuda.get_device_name(0))\n",
    "\n",
    "indexes = list(range(60000))\n",
    "random.shuffle(indexes)\n",
    "valid_frac = .2\n",
    "train_samples_index = indexes[int(valid_frac*len(indexes)):]\n",
    "valid_samples_index = indexes[0:int(valid_frac*len(indexes))]\n",
    "train_loader, valid_loader, test_loader = MNIST_data_loaders(data_loc, train_samples_index, valid_samples_index, kwargs, batch_size)\n",
    "\n",
    "# Create network\n",
    "model = SmallNet()\n",
    "model.cuda()\n",
    "\n",
    "model.reset_all_parameters()\n",
    "lr = schedule[0] #????\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "scheduler = FixedLR(optimizer=optimizer, schedule=schedule)\n",
    "# scheduler = ExponentialLR(optimizer=optimizer, gamma=.1)\n",
    "loss, acc = model.get_lr_performance(optimizer, scheduler, train_loader, valid_loader, epochs, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
