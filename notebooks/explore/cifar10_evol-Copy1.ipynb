{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate/momentum schedules trained from scratch\n",
    "The default to investigate will be resnet50 with CIFAR10\n",
    "\n",
    "Look at a number of epochs just short of where we think it will converge, so we are aiming for a high accuracy, but not taking too long. (Aim for 94% accuracy.)\n",
    "\n",
    "As a compromise between a fully flexible model for learning rates (which won't work with evolutionary algorithms), or using something inflexible like choosing a global learning rate, we choose two learning rates for each epoch, and linearly scale between these during the epoch.\n",
    "\n",
    "Try:\n",
    "* Finding optimal learning rate given common momemtum initialization. \n",
    "* Find optimal momentum given a sensible learning rate schedule.\n",
    "* Try optimizing both at the same time.\n",
    "\n",
    "Maybe:\n",
    "* Look at how the learning rates evolve from a bad initialization to a sensible one (gif)\n",
    "* look at how optimal learning rate schedule changes based on the momemtum used.\n",
    "\n",
    "\n",
    "** Be careful to reinitialize the pytorch/fastai model each time, so we don't start fine tuning an existing model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/rene/Data/learn-lr/src\n",
      "True\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from fastai.imports import *\n",
    "from fastai.transforms import *\n",
    "from fastai.conv_learner import *\n",
    "from fastai.model import *\n",
    "from fastai.dataset import *\n",
    "from fastai.sgdr import *\n",
    "from fastai.plots import *\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "fast_ai_dir = '/media/rene/Data/fastai/'\n",
    "sys.path.append(fast_ai_dir)\n",
    "\n",
    "# ??????????????? this may be causing an error:\n",
    "SEED = 101\n",
    "np.random.seed(SEED)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Add the src directory for functions\n",
    "src_dir = os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), 'src')\n",
    "print(src_dir)\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "# import my functions:\n",
    "from genetic import*\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/media/rene/Data/data/cifar10/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(ni, nf, ks=3, stride=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(ni, nf, kernel_size=ks, bias=False, stride=stride, padding=ks//2),\n",
    "        nn.BatchNorm2d(nf, momentum=0.01),\n",
    "        nn.LeakyReLU(negative_slope=0.1, inplace=True))\n",
    "\n",
    "class ResLayer(nn.Module):\n",
    "    def __init__(self, ni):\n",
    "        super().__init__()\n",
    "        self.conv1=conv_layer(ni, ni//2, ks=1)\n",
    "        self.conv2=conv_layer(ni//2, ni, ks=3)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        return x.add(self.conv2(self.conv1(x)))\n",
    "#        return x.add_(self.conv2(self.conv1(x)))\n",
    "\n",
    "class Darknet(nn.Module):\n",
    "    def make_group_layer(self, ch_in, num_blocks, stride=1):\n",
    "        return [conv_layer(ch_in, ch_in*2,stride=stride)\n",
    "               ] + [(ResLayer(ch_in*2)) for i in range(num_blocks)]\n",
    "\n",
    "    def __init__(self, num_blocks, num_classes, nf=32):\n",
    "        super().__init__()\n",
    "        layers = [conv_layer(3, nf, ks=3, stride=1)]\n",
    "        for i,nb in enumerate(num_blocks):\n",
    "            layers += self.make_group_layer(nf, nb, stride=2-(i==1))\n",
    "            nf *= 2\n",
    "        layers += [nn.AdaptiveAvgPool2d(1), Flatten(), nn.Linear(nf, num_classes)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x): return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phases_linear(lr_sch, mom_sch):\n",
    "    training_phase_schedule = []\n",
    "    for ind in range(0, len(lr_sch), 2):\n",
    "        ind = int(ind)\n",
    "        curr_sch = TrainingPhase(epochs=1, opt_fn=optim.SGD, lr=(lr_sch[ind], lr_sch[ind+1]), lr_decay=DecayType.LINEAR, \n",
    "                          momentum=(mom_sch[ind],mom_sch[ind+1]), momentum_decay=DecayType.LINEAR)\n",
    "        \n",
    "        training_phase_schedule.append(curr_sch)\n",
    "    return training_phase_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_darknet_perf(PATH, lr_sch_list, mom_sch_list, downsample, acc_dict={}, bs=512):\n",
    "    num_workers = 4\n",
    "    sz=32\n",
    "\n",
    "    pop_perf = []\n",
    "\n",
    "    for ind in range(len(lr_sch_list)):\n",
    "        # if schedule already tested, return this acc\n",
    "        if(tuple(lr_sch_list[ind]+mom_sch_list[ind]) in acc_dict):\n",
    "            acc = acc_dict[tuple(lr_sch_list[ind]+mom_sch_list[ind])]\n",
    "            pop_perf.append([acc, lr_sch_list[ind], mom_sch_list[ind]])\n",
    "        else:\n",
    "            stats = (np.array([ 0.4914 ,  0.48216,  0.44653]), np.array([ 0.24703,  0.24349,  0.26159]))\n",
    "            tfms = tfms_from_stats(stats, sz, aug_tfms=[RandomFlip()], pad=32//8)\n",
    "            data = ImageClassifierData.from_paths(PATH, val_name='test', tfms=tfms, bs=bs)\n",
    "\n",
    "            m = Darknet([1, 2, 4, 6, 3], num_classes=10, nf=32)\n",
    "            data = ImageClassifierData.from_paths(PATH, val_name='test', tfms=tfms, bs=bs)\n",
    "            learn = ConvLearner.from_model_data(m, data)\n",
    "            learn.crit = nn.CrossEntropyLoss()\n",
    "            learn.metrics = [accuracy]\n",
    "\n",
    "            learn.fit_opt_sched(phases_linear(lr_sch_list[ind], mom_sch_list[ind]))\n",
    "#             learn.sched.plot_lr(show_text=False)\n",
    "            preds, y = learn.predict_with_targs()\n",
    "            acc = accuracy_np(preds, y)\n",
    "            \n",
    "            pop_perf.append([acc, lr_sch_list[ind], mom_sch_list[ind]])\n",
    "            acc_dict[tuple(lr_sch_list[ind]+mom_sch_list[ind])] = acc\n",
    "            \n",
    "    return pop_perf, acc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_genetic_darknet(PATH, out_loc, generations, epochs, init_lr_sch, init_mom_sch, downsample=1, evolve_lr=True, evolve_mom=False):\n",
    "    \"\"\"Run the genetic algorithm on schedules for number of generations. \n",
    "    Save best model at each generation and final best 5 models\n",
    "    \n",
    "    Take as input lr and mom schedules that are the correct length (=epochs)\n",
    "    \n",
    "    Downsampling is only used for the evolution part. All other parts use the full length of schedules.\n",
    "    \"\"\"\n",
    "    bs=512\n",
    "\n",
    "    lr_sch = init_lr_sch\n",
    "    mom_sch = init_mom_sch\n",
    "\n",
    "    # Store the top schedule and accuracy (tuples) as elements in a list.\n",
    "    history=[]\n",
    "    # same info, but easier format to search:\n",
    "    acc_dict = {}\n",
    "\n",
    "    # Evolve the generation.\n",
    "    for i in range(generations):\n",
    "        print('Running generation: ', i)\n",
    "            \n",
    "        pop_perf, acc_dict_tmp = get_darknet_perf(PATH, lr_sch, mom_sch, downsample, acc_dict, bs)\n",
    "        pop_perf = [x for x in sorted(pop_perf, key=lambda x: x[0], reverse=True)]\n",
    "        history.append(pop_perf)\n",
    "        acc_dict.update(acc_dict_tmp)\n",
    "\n",
    "        # save the intermediate result every generation\n",
    "        out_file = os.path.join(out_loc, 'cifar_darknet_'+'on_gen_'+str(i))\n",
    "        pickle.dump(history, open(out_file, 'wb'))\n",
    "\n",
    "        # print average accuracy, best accuracy, and best schedule\n",
    "        perf_only = [x[0] for x in pop_perf]\n",
    "        avg = sum(perf_only)/len(perf_only)\n",
    "        print('Avg acc: ', avg, 'best acc: ', pop_perf[0][0])\n",
    "        print('LR Schedule: ',[ '%.5f' % elem for elem in pop_perf[0][1]])\n",
    "\n",
    "        # Evolve\n",
    "        lr_perf = [[x[0], x[1]] for x in pop_perf]\n",
    "        mom_perf = [[x[0], x[2]] for x in pop_perf]\n",
    "        if evolve_lr:\n",
    "            # downsample it for evolution\n",
    "            lr_perf = [[x[0], x[1][::downsample]] for x in lr_perf]\n",
    "            lr_sch = evolve(lr_perf, breed_slice)\n",
    "            # upsample it back to normal\n",
    "            lr_sch = [np.repeat(np.array(x), int(downsample)).tolist() for x in lr_sch]\n",
    "        if evolve_mom:\n",
    "            # downsample it for evolution\n",
    "            mom_perf = [[x[0], x[1][::downsample]] for x in mom_perf]\n",
    "            mom_sch = evolve(mom_perf, breed_slice)\n",
    "            # upsample it back to normal\n",
    "            mom_sch = [np.repeat(np.array(x), int(downsample)).tolist() for x in mom_sch]\n",
    "\n",
    "    # get final accuracy, and print the top 5 sorted\n",
    "    pop_perf = get_darknet_perf(PATH, lr_sch, mom_sch, downsample=downsample)\n",
    "    pop_perf = [x for x in sorted(pop_perf, key=lambda x: x[0], reverse=True)]\n",
    "\n",
    "    # Print out the top 5 networks.=\n",
    "    print('Final Results: ', pop_perf[:5])\n",
    "\n",
    "    # save history as a pickle file\n",
    "    out_file = os.path.join(out_loc, 'cifar_dark_'+str(generations)+'_numsch_'+str(epochs)+'_on_gen_'+str(i+29))\n",
    "    pickle.dump(history, open(out_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running generation:  0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9e31398d04444a79d4383f7cad51c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/98 [00:09<00:43,  1.83it/s, loss=2.02]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rene/miniconda3/envs/fastai/lib/python3.6/site-packages/tqdm/_monitor.py:89: TqdmSynchronisationWarning: Set changed size during iteration (see https://github.com/tqdm/tqdm/issues/481)\n",
      "  TqdmSynchronisationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                 \n",
      "    0      1.514411   2.578099   0.1       \n",
      "    1      1.196001   3.016543   0.1                      \n",
      "    2      1.1194     3.412578   0.1981                   \n",
      "    3      0.93135    2.507309   0.3251                    \n",
      "    4      0.740798   0.966175   0.6765                    \n",
      "                                                           \r"
     ]
    }
   ],
   "source": [
    "# now try with bs of 512, instead of 2048. Seems large batch might be less stable?\n",
    "\n",
    "PATH = \"/media/rene/Data/data/cifar10\"\n",
    "out_loc = '/media/rene/Data/data/learn-lr/output/cifar_dark_10epoch_ds1'\n",
    "num_schedules = 12\n",
    "epochs = 10\n",
    "generations = 100\n",
    "downsample = 1\n",
    "\n",
    "# Initialize schedules. Need 2 points for every epoch. \n",
    "size = int(2*epochs/downsample)\n",
    "init_lr_sch = create_population(num_schedules, size=size, rate_range=(-2.5, -1.5))\n",
    "init_mom_sch = [[.9]*size]*num_schedules\n",
    "\n",
    "init_lr_sch = [np.repeat(np.array(x), int(downsample)).tolist() for x in init_lr_sch]\n",
    "init_mom_sch = [np.repeat(np.array(x), int(downsample)).tolist() for x in init_mom_sch]\n",
    "\n",
    "run_genetic_darknet(PATH, out_loc, generations, epochs, init_lr_sch, init_mom_sch,\n",
    "            downsample=downsample, evolve_lr=True, evolve_mom=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
