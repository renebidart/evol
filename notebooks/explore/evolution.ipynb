{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolutionary Algorithms for learning rate schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rbbidart/learn-lr/src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.pyplot import imshow\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.metrics import accuracy_score\n",
    "% matplotlib inline\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.sampler as sampler\n",
    "\n",
    "# Import modules every time \"you run code imported using %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# Add the src directory for functions\n",
    "src_dir = os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), 'src')\n",
    "print(src_dir)\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "# import my functions:\n",
    "%aimport functions\n",
    "from functions import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Tesla P100-PCIE-12GB\n"
     ]
    }
   ],
   "source": [
    "# Set params:\n",
    "base_data_dir = '/home/rbbidart/project/rbbidart/learn-lr/data'\n",
    "cuda = 'True'\n",
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "seed = 101\n",
    "GPU_num = 0\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)    \n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "# torch.cuda.set_device(GPU_num)\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNIST_data_loaders(base_data_dir, train_samples_index, valid_samples_index, batch_size=64):\n",
    "    \"\"\"Make train, validation, test data loaders for MNIST dataset. Limited augmnetation, nothing fancy\n",
    "    \n",
    "    Arguments:\n",
    "        train_samples_index: index of the train samples  ???(what is this index based off???)\n",
    "        valid_samples_index: index of the valid samples\n",
    "        batch_size: \n",
    "    \"\"\"\n",
    "    class ChunkSampler(sampler.Sampler):\n",
    "        \"\"\"Samples elements sequentially from some offset. \n",
    "        \n",
    "        Argument:\n",
    "            samples_index: index of desired samples\n",
    "        \"\"\"\n",
    "        def __init__(self, samples_index):\n",
    "            self.samples_index = samples_index\n",
    "\n",
    "        def __iter__(self):\n",
    "            return iter(self.samples_index)\n",
    "    \n",
    "        def __len__(self):\n",
    "            return len(self.samples_index)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(base_data_dir, train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.RandomRotation(15),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, **kwargs,\n",
    "        sampler=ChunkSampler(train_samples_index))\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(base_data_dir, train=True, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, **kwargs,\n",
    "        sampler=ChunkSampler(valid_samples_index))\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(base_data_dir, train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    return(train_loader, valid_loader, test_loader)\n",
    "\n",
    "\n",
    "class SmallNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "    \n",
    "    def get_lr_performance(self, optimizer, scheduler, train_loader, valid_loader, epochs, verbose=False):        \n",
    "        \"\"\"Return the validation after training for given epochs\"\"\"\n",
    "        \n",
    "        def get_valid_loss():\n",
    "            # Now get the validation loss\n",
    "            self.eval()\n",
    "            valid_loss = 0\n",
    "            correct = 0\n",
    "            num_data=0\n",
    "            for data, target in valid_loader:\n",
    "                num_data+=len(target)\n",
    "                if cuda:\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "                data, target = Variable(data, volatile=True), Variable(target)\n",
    "                output = self(data)\n",
    "                valid_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "                pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "                correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "            valid_loss /= num_data\n",
    "            valid_acc = 100. * correct / num_data\n",
    "            return valid_loss, valid_acc    \n",
    "        \n",
    "        for epoch in range(1, epochs + 1):\n",
    "            # Every epoch step the scheduler\n",
    "            scheduler.step()\n",
    "            self.train()\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                if cuda:\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "                data, target = Variable(data), Variable(target)\n",
    "                optimizer.zero_grad()\n",
    "                output = self(data)\n",
    "                loss = F.nll_loss(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            # to deubg\n",
    "            if verbose:\n",
    "                valid_loss, valid_acc = get_valid_loss()\n",
    "                print('train loss: ', loss.data.cpu().numpy()[0])\n",
    "                print('valid_loss: ', valid_loss, 'valid_acc: ', valid_acc)\n",
    "       \n",
    "        # final validation loss\n",
    "        valid_loss, valid_acc = get_valid_loss()\n",
    "        return valid_loss, valid_acc\n",
    "    \n",
    "    def test(self):\n",
    "        self.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for data, target in test_loader:\n",
    "            if cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            data, target = Variable(data, volatile=True), Variable(target)\n",
    "            output = self(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        test_acc = 100. * correct / len(test_loader.dataset)\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do you properly add another type of LR scheduler?\n",
    "https://github.com/pytorch/pytorch/blob/master/torch/optim/lr_scheduler.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "\n",
    "class _LRScheduler(object):\n",
    "    def __init__(self, optimizer, last_epoch=-1):\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "        if last_epoch == -1:\n",
    "            for group in optimizer.param_groups:\n",
    "                group.setdefault('initial_lr', group['lr'])\n",
    "        else:\n",
    "            for i, group in enumerate(optimizer.param_groups):\n",
    "                if 'initial_lr' not in group:\n",
    "                    raise KeyError(\"param 'initial_lr' is not specified \"\n",
    "                                   \"in param_groups[{}] when resuming an optimizer\".format(i))\n",
    "        self.base_lrs = list(map(lambda group: group['initial_lr'], optimizer.param_groups))\n",
    "        self.step(last_epoch + 1)\n",
    "        self.last_epoch = last_epoch\n",
    "\n",
    "    def get_lr(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "        self.last_epoch = epoch\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "class FixedLR(_LRScheduler):\n",
    "    \"\"\"Learning rate for each epoch corresponds to a learning rate in the list. No decay.\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        schedule (list): List of learning rates. Should be the same as nuber of epochs\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "    Example:\n",
    "        >>> # Assuming 3 epochs, schedule = [0.05, 0.005, 0.0005]\n",
    "        >>> # lr = 0.05     if epoch = 1\n",
    "        >>> # lr = 0.005    if epoch = 2\n",
    "        >>> # lr = 0.0005   if epoch = 3\n",
    "        >>> scheduler = FixedLR(optimizer, schedule = [0.05, 0.005, 0.0005])\n",
    "        >>> for epoch in range(3):\n",
    "        >>>     scheduler.step()\n",
    "        >>>     train(...)\n",
    "        >>>     validate(...)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, schedule, last_epoch=-1):\n",
    "        if not type(schedule) == list:\n",
    "            raise ValueError('Schedule should be a list of'\n",
    "                             ' floats. Got {}', schedule)\n",
    "        self.schedule = schedule\n",
    "        super(FixedLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return self.schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 5\n",
    "# momentum = .5\n",
    "# lr = .1\n",
    "# schedule = np.repeat(lr, epochs).tolist()\n",
    "\n",
    "# indexes = list(range(60000))\n",
    "# random.shuffle(indexes)\n",
    "# valid_frac = .2\n",
    "# train_samples_index = indexes[int(valid_frac*len(indexes)):]\n",
    "# valid_samples_index = indexes[0:int(valid_frac*len(indexes))]\n",
    "# train_loader, valid_loader, test_loader = MNIST_data_loaders(base_data_dir, train_samples_index, valid_samples_index, batch_size=64)\n",
    "\n",
    "# model = SmallNet()\n",
    "# model.cuda()\n",
    "    \n",
    "# optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "# scheduler = FixedLR(optimizer, schedule)\n",
    "\n",
    "# model.get_lr_performance(optimizer, scheduler, train_loader, valid_loader, epochs, verbose=True)\n",
    "# acc, loss = model.test()\n",
    "# print(loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 5\n",
    "# momentum = .5\n",
    "# lr = .001\n",
    "# schedule = np.repeat(lr, epochs).tolist()\n",
    "\n",
    "# indexes = list(range(60000))\n",
    "# random.shuffle(indexes)\n",
    "# valid_frac = .2\n",
    "# train_samples_index = indexes[int(valid_frac*len(indexes)):]\n",
    "# valid_samples_index = indexes[0:int(valid_frac*len(indexes))]\n",
    "# train_loader, valid_loader, test_loader = MNIST_data_loaders(base_data_dir, train_samples_index, valid_samples_index, batch_size=64)\n",
    "\n",
    "# model = SmallNet()\n",
    "# model.cuda()\n",
    "    \n",
    "# optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "# scheduler = FixedLR(optimizer, schedule)\n",
    "\n",
    "# model.get_lr_performance(optimizer, scheduler, train_loader, valid_loader, epochs, verbose=True)\n",
    "# acc, loss = model.test()\n",
    "# print(loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import randint\n",
    "def create_population(num_schedules, epochs=10):\n",
    "    \"\"\" Create Learning rate schedules, called population \n",
    "    \n",
    "    thanks: \"https://blog.coast.ai/lets-evolve-a-neural-network-with-a-genetic-algorithm-code-included-8809bece164\"\n",
    "    Creates learning rate schedules by randomly sampling between 10e1 and 10e-6\n",
    "    Generate a random float in [0, -6], take exp of this\n",
    "    \n",
    "    Args: num_schedules (int): number of random schedules to create\n",
    "          epochs (int): Number of epochs for learning rate schedule\n",
    "    Returns: learning rate schedule\n",
    "    \"\"\"\n",
    "    pop = []\n",
    "    for _ in range(0, num_schedules):\n",
    "        # Create schedule\n",
    "        exponents = np.random.uniform(-6, 0, epochs)\n",
    "        lr_schedule = np.power(10, exponents).tolist()\n",
    "        pop.append(lr_schedule)\n",
    "    return pop\n",
    "\n",
    "def breed(sch1, sch2, way='average', num_children=2):\n",
    "    \"\"\"Make two children as parts of their parents.\n",
    "    \n",
    "    Args:\n",
    "        sch1 (list): lr_schedule\n",
    "        sch2 (list): lr_schedule\n",
    "    \"\"\"\n",
    "    children = []\n",
    "    for _ in range(2):\n",
    "        child = []\n",
    "\n",
    "        # Loop through the parameters and pick params for the kid.\n",
    "        for idx in range(len(sch1)):\n",
    "            if(way=='random'):\n",
    "                child.append(random.choice([sch1[idx], sch2[idx]]))\n",
    "            elif(way=='mean'):\n",
    "                child.append(np.mean(np.array([sch1[idx], sch2[idx]])))\n",
    "        children.append(child)\n",
    "    return children\n",
    "\n",
    "def mutate(lr_schedule):\n",
    "    \"\"\"Randomly mutate one learning rate randomly\n",
    "    \n",
    "    Args:\n",
    "        lr_schedule (list): lr schedule to mutate\n",
    "    \"\"\"\n",
    "    # Choose a random key.\n",
    "    idx = randint(0, len(lr_schedule))\n",
    "    \n",
    "    # Mutate one of the params. Add an amount within a factor of 10 of the original\n",
    "    lr_schedule[idx] = lr_schedule[idx]*random.uniform(-10, 10)\n",
    "    return lr_schedule \n",
    "    \n",
    "def get_population_perf(population):\n",
    "    \"\"\"Evaluate all the schedules for epochs\n",
    "    \n",
    "    Args:\n",
    "        population (list of lists): list schedules which are lists of length epochs\n",
    "        \n",
    "    Returns:\n",
    "        pop_perf: list of tuples indicating the schedule and its' accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create data loaders\n",
    "    indexes = list(range(60000))\n",
    "    random.shuffle(indexes)\n",
    "    valid_frac = .2\n",
    "    train_samples_index = indexes[int(valid_frac*len(indexes)):]\n",
    "    valid_samples_index = indexes[0:int(valid_frac*len(indexes))]\n",
    "    train_loader, valid_loader, test_loader = MNIST_data_loaders(base_data_dir, train_samples_index, valid_samples_index, batch_size=64)\n",
    "\n",
    "    # Create network\n",
    "    model = SmallNet()\n",
    "    model.cuda()\n",
    "    \n",
    "    epochs = len(population[0])\n",
    "    momentum = .5\n",
    "    \n",
    "    pop_perf = []\n",
    "    for curr_schedule in population:\n",
    "        lr = curr_schedule[0] # is this even used?\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "        scheduler = FixedLR(optimizer=optimizer, schedule=curr_schedule)\n",
    "        loss, acc = model.get_lr_performance(optimizer, scheduler, train_loader, valid_loader, epochs, verbose=False)\n",
    "        pop_perf.append((acc, curr_schedule))\n",
    "        \n",
    "    return pop_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolve(pop_perf):\n",
    "    \"\"\"Evolve a population of learning rates. \n",
    "    Args:\n",
    "        pop (list): A list of learning rates\n",
    "    \n",
    "    Process:\n",
    "    1. Tests schedules, and then keeps the top 25%, as well as 10% chance of keeping a poor schedule.\n",
    "    2. Randomly mutate kept networks with 50% prob\n",
    "    3. Fill the ramaining slots in population with children, created by randomly combining the parents \n",
    "        (50%/50% change of averaging a parameter, or randomly selecting one)\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort on the scores.\n",
    "    pop = [x[1] for x in sorted(pop_perf, key=lambda x: x[0], reverse=True)]\n",
    "\n",
    "    # keep the best 25%\n",
    "    retain_length = int(len(pop)*.25)\n",
    "\n",
    "    # The parents are every network we want to keep.\n",
    "    parents = pop[:retain_length]\n",
    "\n",
    "    # For those we aren't keeping, randomly keep some anyway.\n",
    "    for individual in pop[retain_length:]:\n",
    "        if .1 > random.random():\n",
    "            parents.append(individual)\n",
    "\n",
    "    # Randomly mutate some of the networks we're keeping.\n",
    "    for index, individual in enumerate(parents):\n",
    "        if random.random() > .5:\n",
    "            parents[index] = mutate(parents[index])\n",
    "\n",
    "    # Now find out how many spots we have left to fill. (how many children to make )\n",
    "    parents_length = len(parents)\n",
    "    desired_length = len(pop) - parents_length\n",
    "    children = []\n",
    "\n",
    "    # Add children, which are bred from two remaining networks.\n",
    "    while len(children) < desired_length:\n",
    "\n",
    "        # Get a random mom and dad.\n",
    "        male = random.randint(0, parents_length-1)\n",
    "        female = random.randint(0, parents_length-1)\n",
    "\n",
    "        # Assuming they aren't the same network...\n",
    "        if male != female:\n",
    "            male = parents[male]\n",
    "            female = parents[female]\n",
    "\n",
    "            # pick breeding method:\n",
    "            if random.random() > .5:\n",
    "                way='mean'\n",
    "            else:\n",
    "                way = 'random'\n",
    "                \n",
    "            # Breed them.\n",
    "            babies = breed(male, female, way, num_children=2)\n",
    "\n",
    "            # Add the children one at a time.\n",
    "            for baby in babies:\n",
    "                # Don't grow larger than desired length.\n",
    "                if len(children) < desired_length:\n",
    "                    children.append(baby)\n",
    "    parents.extend(children)\n",
    "    return parents\n",
    "\n",
    "def run_genetic(generations, num_schedules, epochs):\n",
    "    \"\"\"Run the genetic algorithm for a given number of generations.\n",
    "\n",
    "    Save the results as .........\n",
    "    Args:\n",
    "        generations (int): Number of times to evole the population\n",
    "        num_schedules (int): number of schedules to use each iteration\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize schedules:\n",
    "    population = create_population(num_schedules=num_schedules, epochs=epochs)\n",
    "\n",
    "    # Evolve the generation.\n",
    "    for i in range(generations):\n",
    "        print('Running generation: ', i)\n",
    "        pop_perf = get_population_perf(population)\n",
    "        pop_perf = [x for x in sorted(pop_perf, key=lambda x: x[0], reverse=True)]\n",
    "        \n",
    "        # print average accuracy, best accuracy, and best schedule\n",
    "        perf_only = [x[0] for x in pop_perf]\n",
    "        avg = sum(perf_only)/len(perf_only)\n",
    "        print('Avg acc: ', avg, 'best acc: ', pop_perf[0][0])\n",
    "        print('Schedule: ',[ '%.5f' % elem for elem in pop_perf[0][1]])\n",
    "\n",
    "        # Evolve\n",
    "        population = evolve(pop_perf, epochs)\n",
    "    \n",
    "    # get final accuracy, and print the top 5 sorted\n",
    "    pop_perf = get_population_perf(population)\n",
    "    pop_perf = [x for x in sorted(pop_perf, key=lambda x: x[0], reverse=True)]\n",
    "\n",
    "    # Print out the top 5 networks.\n",
    "    print('Final Results: ', pop_perf[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running generation:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rbbidart/pytorch/lib/python3.5/site-packages/ipykernel_launcher.py:67: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg acc:  17.341666666666665 best acc:  48.875\n",
      "Schedule:  ['0.00012', '0.00276', '0.00321', '0.03626', '0.00048']\n",
      "Running generation:  1\n",
      "Avg acc:  57.337500000000006 best acc:  81.24166666666666\n",
      "Schedule:  ['0.00006', '0.00149', '0.01328', '0.01821', '0.00025']\n",
      "Running generation:  2\n",
      "Avg acc:  65.828125 best acc:  84.4\n",
      "Schedule:  ['0.00006', '0.00149', '0.01328', '0.01821', '0.00025']\n",
      "Running generation:  3\n",
      "Avg acc:  54.68854166666667 best acc:  80.28333333333333\n",
      "Schedule:  ['0.00006', '0.00149', '0.01328', '0.01821', '0.00025']\n",
      "Running generation:  4\n",
      "Avg acc:  59.87708333333333 best acc:  82.73333333333333\n",
      "Schedule:  ['0.00006', '0.00149', '0.01328', '0.01821', '0.00025']\n",
      "Running generation:  5\n",
      "Avg acc:  56.728125 best acc:  81.04166666666667\n",
      "Schedule:  ['0.00006', '0.00149', '0.01328', '0.01821', '0.00025']\n",
      "Running generation:  6\n",
      "Avg acc:  79.73541666666667 best acc:  89.175\n",
      "Schedule:  ['0.00016', '-0.00082', '0.01328', '0.01821', '0.00025']\n",
      "Running generation:  7\n",
      "Avg acc:  76.03854166666667 best acc:  88.875\n",
      "Schedule:  ['0.00016', '-0.00313', '0.01328', '0.01821', '0.00154']\n",
      "Running generation:  8\n",
      "Avg acc:  74.22395833333334 best acc:  88.44166666666666\n",
      "Schedule:  ['0.00016', '-0.00313', '0.01328', '0.01821', '0.00154']\n",
      "Running generation:  9\n",
      "Avg acc:  75.08437500000001 best acc:  89.25833333333334\n",
      "Schedule:  ['0.00006', '-0.00313', '0.01328', '0.01821', '0.00154']\n",
      "Final Results:  [(86.88333333333334, [6.432462779853419e-05, -0.003127729867515981, 0.013276756136911559, 0.18093839228251893, 0.0015367160698142243]), (85.69166666666666, [6.432462779853419e-05, -0.003127729867515981, 0.013276756136911559, 0.18093839228251893, 0.0015367160698142243]), (84.14166666666667, [6.432462779853419e-05, -0.003127729867515981, 0.013276756136911559, 0.18093839228251893, 0.0015367160698142243]), (82.09166666666667, [6.432462779853419e-05, -0.003127729867515981, 0.013276756136911559, 0.018210459652903733, 0.0015367160698142243]), (78.8, [6.432462779853419e-05, -0.003127729867515981, 0.013276756136911559, 0.18093839228251893, 0.0015367160698142243])]\n"
     ]
    }
   ],
   "source": [
    "run_genetic(generations=10, num_schedules=8, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
